import requests
import csv
from bs4 import BeautifulSoup
import time

# Stałe
BASE_URL = 'https://adresowo.pl'
OUTPUT_FILE = 'ogloszenia_trojmiasto.csv'

# Nagłówki CSV – zgodnie z podanym wzorem
CSV_HEADERS = [
    'id',
    'url',
    'date_posted',
    'photos',
    'locality',
    'street',
    'property_type',
    'rooms',
    'area_m2',
    'owner_direct',
    'price_total_zl',
    'price_per_m2_zl',
    'city'
]

# Nagłówki HTTP
HTTP_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

def _infer_city(locality_text: str, street_text: str) -> str:
    t = f"{locality_text} {street_text}".lower()
    if 'gdańsk' in t or 'gdansk' in t:
        return 'Gdańsk'
    if 'gdynia' in t:
        return 'Gdynia'
    if 'sopot' in t:
        return 'Sopot'
    return ''

def parse_listing(item):
    try:
        header = item.select_one('.result-info__header')
        locality = header.strong.get_text(strip=True) if header and header.strong else ''
        street = header.select_one('.result-info__address').get_text(strip=True) if header and header.select_one('.result-info__address') else ''

        basics = item.select('.result-info__basic:not(.result-info__basic--owner)')
        rooms = basics[0].b.get_text(strip=True) if len(basics) > 0 and basics[0].b else ''
        area_m2 = basics[1].b.get_text(strip=True) if len(basics) > 1 and basics[1].b else ''

        price_total_tag = item.select_one('.result-info__price--total span')
        price_total_zl = price_total_tag.get_text(strip=True).replace('\xa0', '') if price_total_tag else ''

        price_per_m2_tag = item.select_one('.result-info__price--per-sqm span')
        price_per_m2_zl = price_per_m2_tag.get_text(strip=True).replace('\xa0', '') if price_per_m2_tag else ''

        owner_tag = item.select_one('.result-info__basic--owner')
        owner_direct = owner_tag.get_text(strip=True) if owner_tag else 'Pośrednik'

        date_tag = item.select_one('.result-photo__date span')
        date_posted = date_tag.get_text(strip=True) if date_tag else ''

        photos_tag = item.select_one('.result-photo__photos')
        photos = photos_tag.get_text(strip=True) if photos_tag else ''

        link_tag = item.select_one('a')
        url = BASE_URL + link_tag['href'] if link_tag and link_tag.has_attr('href') else ''

        image_tag = item.select_one('.result-photo__image')
        property_type = 'Apartment'
        city = _infer_city(locality, street)

       

        listing_id = link_tag['href'].split('/')[-2] if link_tag and link_tag.has_attr('href') else ''

        return [
            listing_id,
            url,
            date_posted,
            photos,
            locality,
            street,
            property_type,
            rooms,
            area_m2,
            owner_direct,
            price_total_zl,
            price_per_m2_zl,
            city
        ]

    except Exception as e:
        print(f"Błąd podczas parsowania ogłoszenia: {e}")
        return None

def main():
    print(f"Rozpoczynam scraping {BASE_URL}...")
    all_data = []

    with requests.Session() as session:
        session.headers.update(HTTP_HEADERS)

        for page_num in range(1, 11):
            url = f'https://adresowo.pl/f/mieszkania/321532_323296_324637/_l{page_num}'
            print(f"Przetwarzanie strony {page_num}/10: {url}")

            try:
                response = session.get(url, timeout=10)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
                listings = soup.select('section.search-results__item')

                if not listings:
                    print(f"  -> Nie znaleziono ogłoszeń na stronie {page_num}.")
                    break

                print(f"  -> Znaleziono {len(listings)} ogłoszeń.")

                for item in listings:
                    row = parse_listing(item)
                    if row:
                        all_data.append(row)

                time.sleep(0.5)

            except requests.RequestException as e:
                print(f"Błąd podczas pobierania strony {url}: {e}")
                continue

    if all_data:
        print(f"\nZakończono scraping. Zapisywanie {len(all_data)} ogłoszeń do pliku {OUTPUT_FILE}...")
        try:
            with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(CSV_HEADERS)
                writer.writerows(all_data)
            print(f"Pomyślnie zapisano dane w pliku: {OUTPUT_FILE}")
        except IOError as e:
            print(f"Błąd podczas zapisu do pliku {OUTPUT_FILE}: {e}")
    else:
        print("\nNie zebrano żadnych danych.")

if __name__ == "__main__":
    main()